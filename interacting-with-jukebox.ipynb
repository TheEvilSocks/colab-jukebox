{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Interacting with Jukebox",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheEvilSocks/colab-jukebox/blob/master/interacting-with-jukebox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq8uLwZCn0BV"
      },
      "source": [
        "IMPORTANT NOTE ON SYSTEM REQUIREMENTS:\n",
        "\n",
        "If you are connecting to a hosted runtime, make sure it has a P100 GPU (optionally run !nvidia-smi to confirm). Go to Edit>Notebook Settings to set this.\n",
        "\n",
        "CoLab may first assign you a lower memory machine if you are using a hosted runtime.  If so, the first time you try to load the 5B model, it will run out of memory, and then you'll be prompted to restart with more memory (then return to the top of this CoLab).  If you continue to have memory issues after this (or run into issues on your own home setup), switch to the 1B model.\n",
        "\n",
        "If you are using a local GPU, we recommend V100 or P100 with 16GB GPU memory for best performance. For GPUâ€™s with less memory, we recommend using the 1B model and a smaller batch size throughout.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qEqdj8u0gdN"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTr4w6qwO11C"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAMZK4GNA_PM"
      },
      "source": [
        "Mount Google Drive to save sample levels as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPdMgaH_BPGN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Prepare the environment\n",
        "print(\"Preparing environment... (This might take a while)\")\n",
        "!pip install git+https://github.com/openai/jukebox.git > /dev/null\n",
        "print(\"Prepped and ready.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYozwYWu6niI"
      },
      "source": [
        "Setup the configuration with which the AI is gonnna do work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly3FM3SmO1cP"
      },
      "source": [
        "model = \"5b_lyrics\" # \"5b\", \"1b_lyrics\" or \"5b_lyrics\"\n",
        "\n",
        "# Specify where to save the samples.\n",
        "sample_folder = '/content/gdrive/My Drive/Jukebox/Samples'\n",
        "\n",
        "# Specify an audio file here.\n",
        "audio_file = '/content/gdrive/My Drive/Jukebox/Primer.wav'\n",
        "\n",
        "# Specify how many seconds of audio to prime on.\n",
        "prompt_length_in_seconds=12\n",
        "\n",
        "# Specify wether to resume from a previous checkpoint\n",
        "resume_checkpoint = False\n",
        "\n",
        "sample_length_in_seconds = 76          # Full length of musical sample to generate - we find songs in the 1 to 4 minute\n",
        "                                       # range work well, with generation time proportional to sample length.  \n",
        "                                       # This total length affects how quickly the model \n",
        "                                       # progresses through lyrics (model also generates differently\n",
        "                                       # depending on if it thinks it's in the beginning, middle, or end of sample)\n",
        "\n",
        "sampling_temperature = .98\n",
        "\n",
        "\n",
        "song_artist = \"Rick Astley\"\n",
        "song_genre = \"Pop\"\n",
        "song_lyrics = \"\"\"We're no strangers to love\n",
        "You know the rules and so do I\n",
        "A full commitment's what I'm thinking of\n",
        "You wouldn't get this from any other guy\n",
        "\n",
        "I just wanna tell you how I'm feeling\n",
        "Gotta make you understand\n",
        "\n",
        "Never gonna give you up\n",
        "Never gonna let you down\n",
        "Never gonna run around and desert you\n",
        "Never gonna make you cry\n",
        "Never gonna say goodbye\n",
        "Never gonna tell a lie and hurt you\n",
        "\n",
        "We've known each other for so long\n",
        "Your heart's been aching, but\n",
        "You're too shy to say it\n",
        "Inside, we both know what's been going on\n",
        "We know the game and we're gonna play it\n",
        "\n",
        "And if you ask me how I'm feeling\n",
        "Don't tell me you're too blind to see\n",
        "\n",
        "Never gonna give you up\n",
        "Never gonna let you down\n",
        "Never gonna run around and desert you\n",
        "Never gonna make you cry\n",
        "Never gonna say goodbye\n",
        "Never gonna tell a lie and hurt you\n",
        "\n",
        "Never gonna give you up\n",
        "Never gonna let you down\n",
        "Never gonna run around and desert you\n",
        "Never gonna make you cry\n",
        "Never gonna say goodbye\n",
        "Never gonna tell a lie and hurt you\n",
        "\n",
        "(Ooh, give you up)\n",
        "(Ooh, give you up)\n",
        "Never gonna give, never gonna give\n",
        "(Give you up)\n",
        "Never gonna give, never gonna give\n",
        "(Give you up)\n",
        "\n",
        "We've known each other for so long\n",
        "Your heart's been aching, but\n",
        "You're too shy to say it\n",
        "Inside, we both know what's been going on\n",
        "We know the game and we're gonna play it\n",
        "\n",
        "I just wanna tell you how I'm feeling\n",
        "Gotta make you understand\n",
        "\n",
        "Never gonna give you up\n",
        "Never gonna let you down\n",
        "Never gonna run around and desert you\n",
        "Never gonna make you cry\n",
        "Never gonna say goodbye\n",
        "Never gonna tell a lie and hurt you\n",
        "\n",
        "Never gonna give you up\n",
        "Never gonna let you down\n",
        "Never gonna run around and desert you\n",
        "Never gonna make you cry\n",
        "Never gonna say goodbye\n",
        "Never gonna tell a lie and hurt you\n",
        "\n",
        "Never gonna give you up\n",
        "Never gonna let you down\n",
        "Never gonna run around and desert you\n",
        "Never gonna make you cry\n",
        "Never gonna say goodbye\n",
        "Never gonna tell a lie and hurt you\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89FftI5kc-Az"
      },
      "source": [
        "# Setup the AI\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4COGVm8b7uLl"
      },
      "source": [
        "Just run this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65aR2OZxmfzq"
      },
      "source": [
        "import jukebox\n",
        "import torch as t\n",
        "import librosa\n",
        "import os\n",
        "from IPython.display import Audio\n",
        "from jukebox.make_models import make_vqvae, make_prior, MODELS, make_model\n",
        "from jukebox.hparams import Hyperparams, setup_hparams\n",
        "from jukebox.sample import sample_single_window, _sample, \\\n",
        "                           sample_partial_window, upsample, \\\n",
        "                           load_prompts\n",
        "from jukebox.utils.dist_utils import setup_dist_from_mpi\n",
        "from jukebox.utils.torch_utils import empty_cache\n",
        "rank, local_rank, device = setup_dist_from_mpi()\n",
        "\n",
        "\n",
        "\n",
        "hps = Hyperparams()\n",
        "hps.sr = 44100\n",
        "hps.n_samples = 8 if model=='1b_lyrics' else 3\n",
        "# Specifies the directory to save the sample in.\n",
        "# We set this to the Google Drive mount point.\n",
        "hps.name = sample_folder\n",
        "chunk_size = 32 if model==\"1b_lyrics\" else 16\n",
        "max_batch_size = 16 if model==\"1b_lyrics\" else 3\n",
        "hps.levels = 3\n",
        "hps.hop_fraction = [.5,.5,.125]\n",
        "\n",
        "vqvae, *priors = MODELS[model]\n",
        "vqvae = make_vqvae(setup_hparams(vqvae, dict(sample_length = 1048576)), device)\n",
        "top_prior = make_prior(setup_hparams(priors[-1], dict()), vqvae, device)\n",
        "\n",
        "# Prime song creation using an arbitrary audio sample.\n",
        "mode = 'primed'\n",
        "codes_file=None\n",
        "\n",
        "if resume_checkpoint:\n",
        "  if os.path.exists(hps.name):\n",
        "    # Identify the lowest level generated and continue from there.\n",
        "    for level in [1, 2]:\n",
        "      data = f\"{hps.name}/level_{level}/data.pth.tar\"\n",
        "      if os.path.isfile(data):\n",
        "        mode = 'upsample'\n",
        "        codes_file = data\n",
        "        print('Upsampling from level '+str(level))\n",
        "        break\n",
        "print('mode is now '+mode)\n",
        "\n",
        "sample_hps = Hyperparams(dict(mode=mode, codes_file=codes_file, audio_file=audio_file, prompt_length_in_seconds=prompt_length_in_seconds))\n",
        "\n",
        "\n",
        "hps.sample_length = (int(sample_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
        "assert hps.sample_length >= top_prior.n_ctx*top_prior.raw_to_tokens, f'Please choose a larger sampling rate'\n",
        "\n",
        "# Note: Metas can contain different prompts per sample.\n",
        "# By default, all samples use the same prompt.\n",
        "metas = [dict(artist = song_artist,\n",
        "            genre = song_genre,\n",
        "            total_length = hps.sample_length,\n",
        "            offset = 0,\n",
        "            lyrics = song_lyrics,\n",
        "            ),\n",
        "          ] * hps.n_samples\n",
        "labels = [None, None, top_prior.labeller.get_batch_labels(metas, 'cuda')]\n",
        "\n",
        "lower_batch_size = 16\n",
        "max_batch_size = 16 if model == \"1b_lyrics\" else 3\n",
        "lower_level_chunk_size = 32\n",
        "chunk_size = 32 if model == \"1b_lyrics\" else 16\n",
        "sampling_kwargs = [dict(temp=.99, fp16=True, max_batch_size=lower_batch_size,\n",
        "                        chunk_size=lower_level_chunk_size),\n",
        "                    dict(temp=0.99, fp16=True, max_batch_size=lower_batch_size,\n",
        "                         chunk_size=lower_level_chunk_size),\n",
        "                    dict(temp=sampling_temperature, fp16=True, \n",
        "                         max_batch_size=max_batch_size, chunk_size=chunk_size)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3j0gT3HfrRD"
      },
      "source": [
        "# Start the generation process\n",
        "\n",
        "Now we're ready to sample from the model. We'll generate the top level (2) first, followed by the first upsampling (level 1), and the second upsampling (0).  In this CoLab we load the top prior separately from the upsamplers, because of memory concerns on the hosted runtimes. If you are using a local machine, you can also load all models directly with make_models, and then use sample.py's ancestral_sampling to put this all in one step.\n",
        "\n",
        "After each level, we decode to raw audio and save the audio files.   \n",
        "\n",
        "This next cell will take a while (approximately 10 minutes per 20 seconds of music sample)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a1tlvcVlHhN"
      },
      "source": [
        "if sample_hps.mode == 'upsample':\n",
        "  assert sample_hps.codes_file is not None\n",
        "  # Load codes.\n",
        "  data = t.load(sample_hps.codes_file, map_location='cpu')\n",
        "  zs = [z.cuda() for z in data['zs']]\n",
        "  assert zs[-1].shape[0] == hps.n_samples, f\"Expected bs = {hps.n_samples}, got {zs[-1].shape[0]}\"\n",
        "  del data\n",
        "  print('Falling through to the upsample step later in the notebook.')\n",
        "elif sample_hps.mode == 'primed':\n",
        "  assert sample_hps.audio_file is not None\n",
        "  audio_files = sample_hps.audio_file.split(',')\n",
        "  duration = (int(sample_hps.prompt_length_in_seconds*hps.sr)//top_prior.raw_to_tokens)*top_prior.raw_to_tokens\n",
        "  x = load_prompts(audio_files, duration, hps)\n",
        "  zs = top_prior.encode(x, start_level=0, end_level=len(priors), bs_chunks=x.shape[0])\n",
        "  zs = _sample(zs, labels, sampling_kwargs, [None, None, top_prior], [2], hps)\n",
        "else:\n",
        "  raise ValueError(f'Unknown sample mode {sample_hps.mode}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xt87rQv6R3j"
      },
      "source": [
        "# Upsampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJc3bQxmusc6"
      },
      "source": [
        "We are now done with the large top_prior model, and instead load the upsamplers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH_jUhGDprAt"
      },
      "source": [
        "Please note: this next upsampling step will take several hours.  At the free tier, Google CoLab lets you run for 12 hours.  As the upsampling is completed, samples will appear in the Files tab (you can access this at the left of the CoLab), under \"samples\" (or whatever hps.name is currently).  Level 1 is the partially upsampled version, and then Level 0 is fully completed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5VLX0zRapIm"
      },
      "source": [
        "# Set this False if you are on a local machine that has enough memory (this allows you to do the\n",
        "# lyrics alignment visualization during the upsampling stage). For a hosted runtime, \n",
        "# we'll need to go ahead and delete the top_prior if you are using the 5b_lyrics model.\n",
        "if True:\n",
        "  del top_prior\n",
        "  empty_cache()\n",
        "  top_prior=None\n",
        "upsamplers = [make_prior(setup_hparams(prior, dict()), vqvae, 'cpu') for prior in priors[:-1]]\n",
        "labels[:2] = [prior.labeller.get_batch_labels(metas, 'cuda') for prior in upsamplers]\n",
        "\n",
        "# The actual upsampling bit\n",
        "zs = upsample(zs, labels, sampling_kwargs, [*upsamplers, top_prior], hps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SJgBYJPri55"
      },
      "source": [
        "Listen to your final sample!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ip2PPE0rgAb"
      },
      "source": [
        "Audio(f'{hps.name}/level_0/item_0.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JAgFxytwrLG"
      },
      "source": [
        "del upsamplers\n",
        "empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}